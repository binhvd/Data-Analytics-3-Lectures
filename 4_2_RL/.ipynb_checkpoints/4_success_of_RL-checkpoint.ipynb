{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ed11a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Success of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9b17a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backgammon\n",
    "\n",
    "A computer version of the backgammon was developed by Gerald Tesauro at IBM's Thomas J. Watson Research Center in 1992. They called it TD-gammon.\n",
    "\n",
    "They developed a TD-learning (temporal difference learning algorithm) for learning a good strategy for playing the game. We will discuss TD-learning later on. \n",
    "\n",
    "They managed to achieve the level of human players."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d623d03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The board of the game:\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1KY6VwerZxL1Vjq-cReemeja-oK-IocOD\" width=45%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb547a3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Characteristics of the game:\n",
    "\n",
    "* complexity of the game\n",
    "    * number of states: $10^{20}$\n",
    "    * branching factor: about $400$\n",
    "* stochasticity of the opponent\n",
    "* easy to simulate the steps on the board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a773ea9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Flying a helicopter\n",
    "\n",
    "[A helicopter learns to fly](https://www.youtube.com/watch?v=M-QUkgk3HyE)\n",
    "\n",
    "Peter Abbeel and Andrew Ng in 2008.\n",
    "\n",
    "Previous works by Andrew Ng to train a helicopter to perform complicated maneuvers. He developed the algorithm called [Pegasus](https://arxiv.org/abs/1301.3878).\n",
    "\n",
    "Characteristics of the problem:\n",
    "* continuous control signals\n",
    "* difficult to model and understand the reaction of the helicopter\n",
    "* aerodinamical uncertainty (and uncertainty)\n",
    "* real-time interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d720629",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Atari games\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1Gs6s2riwVswUSPyvg75fIeK3GnBmZiIV\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62888143",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Important breakthrough in the history of reinforcement learning. This was the first time when deep neural networks were combined with reinforcement learning successfully (DQN and its variants, see later on). This achievement was in 2013-2015 with several similar papers.\n",
    "\n",
    "The learning is done by observing only the raw frames (with a slight preprocessing) and the scores achieved in the game. The rules are not known and the exact state of the game engine is not known too. The algorithm has to deduce those information from the image sequence and the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e0e74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The algorithm was able to achieve human-level performance on more than half of the Atari games (more than 50 games exist) with the same hyper-paramteres. The algorithm was trained for each game separately but with the same settings. [Video playing Atari](https://www.youtube.com/watch?v=V1eYniJ0Rnk&ab_channel=TwoMinutePapers)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1i2hn7vDsYdyrr6BRdktmynzeRB5tKyt4\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377df4e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Characterictics of the task:\n",
    "* the number of states: $10^{12000}$\n",
    "* the number of actions are small (2-10)\n",
    "* only the frames can be seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af21590",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Playing Go on human-level and beyond\n",
    "\n",
    "\"Go is an abstract strategy board game for two players, in which the aim is to surround more territory than the opponent. The game was invented in China more than 2,500 years ago and is believed to be the oldest board game continuously played to the present day. A 2016 survey by the International Go Federation's 75 member nations found that there are over 46 million people worldwide who know how to play Go and over 20 million current players\"\n",
    "\n",
    "-- [Wikipedia article](https://en.wikipedia.org/wiki/Go_(game))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6de610",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1mtEp02WIb9d4Ww1KU9_UKtM45gZM2dP6\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912b094",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In October 2015, AlphaGo played its first match against the reigning three-time European Champion, Mr Fan Hui. AlphaGo won the first ever game against a Go professional with a score of 5-0.\n",
    "\n",
    "[AlphaGo vs Fan Hui](https://en.wikipedia.org/wiki/AlphaGo_versus_Fan_Hui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8774bc3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "AlphaGo then competed against legendary Go player Mr Lee Sedol, the winner of 18 world titles, who is widely considered the greatest player of the past decade. AlphaGo's 4-1 victory in Seoul, South Korea, on March 2016 was watched by over 200 million people worldwide. This landmark achievement was a decade ahead of its time.\n",
    "\n",
    "[AlphaGo vs Lee Sedol](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01c3201",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In January 2017, DeepMind revealed an improved, online version of AlphaGo called Master. This online player achieved 60 straight wins in time-control games against top international players. \n",
    "\n",
    "[AlphaGo vs AlphaGo](https://deepmind.com/alphago-vs-alphago)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd126541",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Four months later, AlphaGo took part in the Future of Go Summit in China, the birthplace of Go. Following the summit, Deepmind revealed **AlphaGo Zero**. While AlphaGo learnt the game by playing thousands of matches with amateur and professional players, AlphaGo Zero learnt by playing against itself, starting from completely random play.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/900/1*MlwJE5cDvc3WL76v_ViERw.gif\" width=55%>\n",
    "\n",
    "-- [Source](https://medium.com/predict/a-step-towards-agi-the-story-of-alphago-e0fafd83e6b9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d2e99",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deepmind developed a more advanced version, called **MuZero**. It is a remarkable milestone in RL because it is able to achieve high results on Chess, Shogi, Go and Atari! \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/653/1*EsI4_4cfhtUi-iAgaUeyvQ.png\" width=55%>\n",
    "\n",
    "-- [Source](https://towardsdatascience.com/deepmind-unveils-muzero-a-new-agent-that-mastered-chess-shogi-atari-and-go-without-knowing-the-d755dc80ff08)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7566f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Characteristics of the game:\n",
    "* a lot of possible sequences of moves: $250^{150}$\n",
    "* board can be seen, moves are well defined\n",
    "* opponent is stochastic\n",
    "* the game is complex, requires intuition and creativity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25ba5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References:**\n",
    "\n",
    "[Story of AlphaGO](https://deepmind.com/research/case-studies/alphago-the-story-so-far)\n",
    "\n",
    "[Mastering the game of Go paper](https://www.nature.com/articles/nature16961.pdf)\n",
    "\n",
    "[MuZero paper](https://arxiv.org/abs/1911.08265)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7415026",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Playing Dota2\n",
    "\n",
    "Dota2 is a really complex game. It requires strategies, the controlling of a team based on collaboration. There are a diverse set of actions. The strategies should take into account the future, this means a long-term horizon. It is very hard for an RL agent to be proactive.\n",
    "\n",
    "Successful approach was OpenAI Five."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16067b51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"We started working with Dota 2 because we expected it to be a good testbed for developing general-purpose AI technologies. It has additionally turned out to be a great avenue for helping people experience modern AI — which we expect to become a high-stakes part of people’s lives in the future, starting with systems like self-driving cars.\"\n",
    "\n",
    "-- [Source](https://openai.com/blog/openai-five-finals/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e246a56e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Game rules and goal**\n",
    "\n",
    "* 5 heroes on the field\n",
    "* each hero has 4 abilities\n",
    "* objective: destroy the ancient of the enemy\n",
    "* 2 sides: radiant, dire\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1c9LlTjAMSDESEtxQrhH6cjCASO6ROS3K\" width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60e5bc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1bqs42fr2REpMH2p0R5zTdytOnEK1vVim\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f2049f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Dota2 gameplay](https://www.youtube.com/watch?time_continue=1&v=UZHTNBMAfAA&feature=emb_logo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289fd4d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "During training an enormous power was used:\n",
    "\n",
    "* 128000 CPU cores\n",
    "* 256 P100 GPUs\n",
    "* 10000 years worth of experience (180 years per day)\n",
    "* 64.000 watts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8636fb81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OpenAI Five wins back-to-back games versus Dota 2 world champions OG at Finals, becoming the first AI to beat the world champions in an esports game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d09a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References:**\n",
    "\n",
    "[Official webpage](https://openai.com/projects/five/)\n",
    "\n",
    "[Dota2 paper](https://cdn.openai.com/dota-2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f3094",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Playing Starcraft\n",
    "\n",
    "Similar game like Dota2. The goal and the difficulty are similar as well. The project is maintained by Deepmind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b61e1d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"AlphaStar is the first AI to reach the top league of a widely popular esport without any game restrictions. This January, a preliminary version of AlphaStar challenged two of the world's top players in StarCraft II, one of the most enduring and popular real-time strategy video games of all time. Since then, we have taken on a much greater challenge: playing the full game at a Grandmaster level under professionally approved conditions.\"\n",
    "\n",
    "-- [Source](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459799a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://fudzilla.com/media/k2/items/cache/40f24d728e5f4849828b04937e4b92a2_L.jpg\" width=45%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04e8dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How AlphaStar improved over time until it has beaten top players of the world:\n",
    "\n",
    "<img src=\"https://cdn.vox-cdn.com/thumbor/I0h7LHOwssieeS_lYYif7LYJqiU=/0x0:1440x810/920x0/filters:focal(0x0:1440x810):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/19332213/Figure_3_static.jpg\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e0fa8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Characteristics of the game:\n",
    "* long-term horizon\n",
    "* partially-observable environment\n",
    "* a lot of possible actions\n",
    "* a lot of possible situations, decision sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f6ba9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**References:**\n",
    "\n",
    "[AlphaStar](https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning)\n",
    "\n",
    "[API](https://github.com/deepmind/pysc2)\n",
    "\n",
    "[AI master online](https://www.nature.com/articles/d41586-019-03343-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7b233",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real world applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313c9a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decreasing energy consumption in Google data centers\n",
    "\n",
    "[Energy consumption decreasing](https://sustainability.google/projects/machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12707c1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nascar racing\n",
    "\n",
    "[Nascar racing example](https://youtu.be/lrv8ga02VNg?t=852)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e2030a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Osaro\n",
    "\n",
    "[osaro](https://www.osaro.com)\n",
    "\n",
    "Uses RL for:\n",
    "* picking tasks\n",
    "* assembly tasks\n",
    "* creating other robots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0c319",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Covariant\n",
    "\n",
    "[covariant](https://covariant.ai/)\n",
    "\n",
    "Uses RL for robotics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f87a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bonsai\n",
    "\n",
    "[bonsai](https://www.bons.ai/)\n",
    "\n",
    "Creates an RL platform for industrial usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea2fab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Boston Dynamics\n",
    "\n",
    "[Boston Dynamics](https://www.bostondynamics.com/atlas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2704d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Facebook's Horizon\n",
    "\n",
    "[Horizon](https://research.fb.com/publications/horizon-facebooks-open-source-applied-reinforcement-learning-platform/)\n",
    "\n",
    "RL platform, opensource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ffbab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Waymo\n",
    "\n",
    "[waymo](https://waymo.com/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b889dea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### J.P.Morgan\n",
    "\n",
    "[RL for foreign exchange](https://www.jpmorgan.com/global/markets/machine-learning-fx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
