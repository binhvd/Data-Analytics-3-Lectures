{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d90fcf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RL formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172f9ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definitions, basic terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da499338",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RL framework\n",
    "\n",
    "Objective is to gather as much reward as possible during operation.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1VqFRFYo8Tv2YDG__6ntVliUNMb-RlEmU\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613f400",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RL framework\n",
    "\n",
    "Reinforcement learning uses interactions for learning. As opposed to supervised learning, the dataset to learn from (whether labeled or not) is unavailable. The agent generates the dataset (experiences) during the learning process by interactions. \n",
    "The agent can influence its environment by actions. The effect of the action on the environment is described by the states. \n",
    "The agent receives feedback signals from the environment, which indicates the efficiency of its actions.\n",
    "The goal of the agent is to gather the most rewards possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed46d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Environment and agent\n",
    "\n",
    "In RL we have two players, interacting with each other: the environment and the agent. \n",
    "\n",
    "Examples:\n",
    "* Drone: agent - flying drone, environment - its surrounding\n",
    "* Trading bots: agent - the trading algorithm, environment - stock market (the broker interface)\n",
    "* AlphGo: agent - the algorithm playing the game, environment - board and the opponent (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56eedde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of state\n",
    "\n",
    "The mathematical framework of reinforcement learning is based on the so called Markov Decision Process (MDP). More details later on. One of the key part of MDP, is the state. The state should describe the environment fully, meaning there are no further information required to determine the future of the environment.\n",
    "\n",
    "Example 1: Consider the following physical system. We have an inclined slope with slope angle 45 degrees. There is a ball on the slope and its heading toward the bottom of the slope. In order to describe the dynamical behavior of the system we need the **position** and the **velocity** of the ball (and the parameters of the ball but it is assumed to be fix over time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d1fd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of state\n",
    "\n",
    "Example 1 (continue): Therefore the state of this environment (the pyhysical system) is the position and the velocity of the ball. This is ensured by the second law of Newton because the equation does not contain higher derivatives of two (acceleration):\n",
    "\n",
    "$$F = m \\cdot \\frac{d^2 s}{d t^2}$$\n",
    "\n",
    "Therefore by knowing the position and the  velocity at a moment, the future positions and velocities (future states) can be determined by the laws of physics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02828b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of state\n",
    "\n",
    "Example 2: In case of the drone. The state is represented by the image (current frame) created by the camera. \n",
    "\n",
    "*Generally: a state is well chosen if it is able to describe the environment by its own, there is no need to know further hystorical data. The future can be calculated by knowing the state and the dynamics of the environment.*\n",
    "\n",
    "What can we tell about the state in the second example. Is it enough to fully describe the environment? Can you show examples when it is not enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bdd8e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of action\n",
    "\n",
    "The agent can change the state of the environment by the actions. The actions can be represented by vectors. \n",
    "\n",
    "Examples:\n",
    "\n",
    "* Drone: changing the trust power of the rotors\n",
    "* Trading bots: sell, buy, wait, choose the stock\n",
    "* AlphaGo: putting a stone to the board"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0771271",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of rewards\n",
    "\n",
    "The environment signals rewards to the agent indicating its results. To distinguish the gathered rewards so far from the rewards received in a moment, the literature uses the return and the immediate reward terms.\n",
    "\n",
    "Because the reward is signaled by the environment, the generation of reward is also part of the model of the environment. Therefore we can use a probability distribution to describe the rewards: $P(r|s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ad269c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of rewards\n",
    "\n",
    "Summed return:\n",
    "\n",
    "$$G = \\sum_i{r_i}$$\n",
    "\n",
    "Discounted return:\n",
    "\n",
    "$$G = \\sum_i{\\gamma^i r_i}$$\n",
    "where $\\gamma < 1$ and it is assumed that $r_i < R_{max}$, $\\forall i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c80204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of rewards\n",
    "\n",
    "Because of this last one, if $i\\rightarrow \\infty$:\n",
    "\n",
    "$$G = \\sum_i{\\gamma^i r_i} < R_{max} \\sum_i{\\gamma^i} = \\frac{R_{max}}{1-\\gamma}$$\n",
    "therefore $G$ remains finite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca477fb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of policy\n",
    "\n",
    "The agent's behavior is represented by the policy. The policy is a function that maps the states to actions (deterministic policy) or to action probabilities (stochastic). $\\pi: S \\rightarrow A$ or $\\pi: S \\rightarrow P(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805986f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of policy\n",
    "\n",
    "Deterministic policy:\n",
    "\n",
    "$$a = \\pi(s)$$\n",
    "\n",
    "Stochastic policy:\n",
    "\n",
    "$$\\pi(s, a) = P(a|s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d9b131",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of policy\n",
    "\n",
    "Stochastic policies can have advantages over deterministic ones.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1b-EDUk5cFVpqtOvZ0o4begzKCYwO0dMS\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adde7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of policy\n",
    "\n",
    "If the environment is partially observable then only some parts of the state can be seen. If the state is $s = [f_1, f_2 ..., f_n]$, where $f_i$ is a parameter of the state, the observation can be: $o = [f_1, f_2, ..., f_m]$, $m < n$. \n",
    "\n",
    "The gridworld example: each grid is a state but the agent sees only the walls, exactly at the borders. So on the grids with quotion marks, the agent will see a wall at the bottom and top. As a consequence the two states have the same observations. They can not be distinguished. However, the first case requires to go to the right, but the second case requires to go to the left. \n",
    "\n",
    "A deterministic policy is not enough because it works when we are lucky regarding the initial state. A stochastic policy picks left and right with equal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a97ba39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of transition matrix\n",
    "\n",
    "Idea: The transition matrix describes the probability of the state transitions of the environment. This can describe the dynamics of the environment. Formally:\n",
    "\n",
    "$$T(s, s') = P(s'|s)$$\n",
    "\n",
    "Because in each state it is sure that something will happen:\n",
    "\n",
    "$$\\sum_i{T(s, s_i)}=1.0, \\forall s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb613713",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of transition matrix\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1Bjtd-5U4ixFJVh9yLDBlmXw8UYn3BV9i\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276c8de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of transition matrix\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1xYKAW6u_phry9BJEPifw1_mGYdWgvDGS\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668cffa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of transition matrix\n",
    "\n",
    "Examples:\n",
    "\n",
    "* transition from $s_1$ -> $s_2$: $T(s_1, s_2) = 1.0$\n",
    "* transition from $s_4$ -> $s_3$: $T(s_4, s_3) = 0.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc889fa0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition of transition matrix\n",
    "\n",
    "The transition matrix is a bit more complicated for reinforcement learning because the transition also depends on the actions:\n",
    "\n",
    "$$T(s, a, s') = P(s'|s, a)$$\n",
    "\n",
    "Because in each state it is sure that something will happen after an action was applied:\n",
    "\n",
    "$$\\sum_i{T(s, a, s_i)}=1.0, \\forall s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f6b0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trajectory, generated by policy $\\pi$\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1cjBsgkWXO-jlJRMjV7QDuqODs4-IRcCU\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48694880",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "The Markov Decision Process is defined by the following:\n",
    "\n",
    "* S - a finite set of states\n",
    "* A - a finite set of actions\n",
    "* T - transition probability\n",
    "* R - reward function, given as $R(s, a) = E\\left[ r | s, a \\right]$\n",
    "* $\\gamma$ - discounting factor (if the return is discounted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99d96f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "Markov property:\n",
    "\n",
    "$$P\\left( X_t | X_{t-1} \\right) = P\\left( X_t| X_{t-1}, X_{t-2}, ..., X_0 \\right)$$\n",
    "\n",
    "In words: The future is independent of the past given the present.\n",
    "\n",
    "Markov chain:\n",
    "\n",
    "If we have a $X_0, X_1, ..., X_t$ random variable series and the Markov property holds for each time, this is a discrete-time Markov chain. Think about examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cd921",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "The trajectory shown earlier is a Markov chain. The Markov property:\n",
    "\n",
    "$$P\\left( s_t|s_{t-1}, a_{t-1} \\right) = P\\left( s_t | s_{t-1}, a_{t-1}, s_{t-2}, a_{t-2}, ... s_0, a_0 \\right)$$\n",
    "\n",
    "The first term is the transition probability, introduced earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105020e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Environment types\n",
    "* fully observable, partially observable\n",
    "* episodic, continuing\n",
    "* stationary, non-stationary\n",
    "* continuous action space, discrete action space\n",
    "* stochastic environment, deterministic environment\n",
    "* sparse reward, dense reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8569b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Fully-observable, partially-observable**\n",
    "\n",
    "The environment is fully observable if the state is able to describe the environment (the Markov property holds) fully, otherwise it is partially observable. Therefore the main difference between the state and the observation is that for the states the Markov property holds but for the observations do not.\n",
    "\n",
    "Example:\n",
    "\n",
    "* fully - in case of a video game if I know the inner state of the game engine (all of the positions, status etc.)\n",
    "* partially - in case of a video game (like Dota2) I see only the screen but I do not know anything about the other parts of the space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3a71b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Episodic, continuing (tasks)**\n",
    "\n",
    "An environment is episodic if there are terminating states (where the Markov chain ends and can not go ahead anymore) and they are achievable (ergodicity). A continuing environment means if we start a process, it lasts forever.\n",
    "\n",
    "Example:\n",
    "* episodic - board games, parking a car, manouvering a helicopter from A to B\n",
    "* continuing - trading bots, keeping the accumulator charged of a robot in a warehouse, recommendation engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c300feaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Stationary, non-stationary**\n",
    "\n",
    "An environment is stationary if its dynamics (behavior) do not change with time, the transition matrix and reward function are constant in time. Otherwise it is non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b295b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Continuous, discrete action space**\n",
    "\n",
    "Example:\n",
    "\n",
    "* continuous - training a robot to walk, the action is the set of forces which should be applied to move legs and other parts, the forces can be any real number\n",
    "* discrete - game of Pacman, the possible actions are discrete (left, right, up and down)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80d9af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Stochastic, deterministic**\n",
    "\n",
    "If the dynamics of the environment shows stochasticity, the transition matrix contains elements other than 1.0, the environment is stochastic. Otherwise it is deterministic. In other words, the environment is deterministic if starting from the same state and doing the same thing, the result will be the same.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Deterministic - controlling a robot (assuming proper physical conditions)\n",
    "* Stochastic - trading, drone flight when wind blows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb7e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Sparse, dense reward**\n",
    "\n",
    "Most of the time rewards are recieved rarely (e.g. at the end of the episode). This makes the learning process more difficult. In an ideal case, the reward is dense, meaning it is received frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d6a15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4f190",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the previous part we discussed N-armed bandits which are basically, stateless decision problems. Now we are discussing an important family of algorithms where the model of the environment is perfectly known, we have several states but a simulator is available. The simulator makes it possible to go ahead and simulate the decisions. Based on the \"imagined\" experiences the agent can make a real decision. \n",
    "\n",
    "This is similar when someone plays a board game and tries to think ahead in time, plays the game in head then chooses the next step.\n",
    "\n",
    "MCTS stands for: Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbffe6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MCTS is proved to be successful in games like Go, Chess, Shoggi. Furthermore it was used for Atari-games (see later in the course) when the state of the game (so called ROM) is known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ba392",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MCTS is an umbrella name for algorithms that has the following structure (Sutton book):\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1gOEkDBksko9kvp-JmlDfvUyp_iPK7pcn\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943f998e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Selection:** Starting at the root node, a tree policy based on the action values attached to the\n",
    "edges of the tree traverses the tree to select a leaf node.\n",
    "\n",
    "**Expansion:** On some iterations (depending on details of the application), the tree is expanded\n",
    "from the selected leaf node by adding one or more child nodes reached from the selected node via\n",
    "unexplored actions.\n",
    "\n",
    "**Simulation:** From the selected node, or from one of its newly-added child nodes (if any), simulation\n",
    "of a complete episode is run with actions selected by the rollout policy. The result is a\n",
    "Monte Carlo trial with actions selected first by the tree policy and beyond the tree by the rollout\n",
    "policy.\n",
    "\n",
    "**Backup:** The return generated by the simulated episode is backed up to update, or to initialize,\n",
    "the action values attached to the edges of the tree traversed by the tree policy in this iteration\n",
    "of MCTS. No values are saved for the states and actions visited by the rollout policy beyond the\n",
    "tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f02f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=13xVD0_lTw77CXwriSq2iLmF_uOjFhNjC\" width=75%>\n",
    "\n",
    "\n",
    "[source](https://stanford.edu/~rezab/classes/cme323/S15/projects/montecarlo_search_tree_report.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42457849",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### UCT\n",
    "\n",
    "We put special attention of a concrete MCTS algorithm, the so-called UCT. UCT stands for upper-confidence bounds for trees.\n",
    "First proposed in [Levente Kocsis and Csaba Szepesvari, Bandit based Monte-Carlo Planning](http://ggp.stanford.edu/readings/uct.pdf).\n",
    "\n",
    "The main result of this paper is that they showed, the UCT algorithm chooses the optimal action with probability 1, if the number of samples goes to infinity (consistence). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72b4ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The algorithm maintains a score in each node:\n",
    "\n",
    "$$Score(s') = U(s') + \\beta \\sqrt{\\frac{\\log n(s)}{n(s')}}$$\n",
    "\n",
    "First term: exploitation term (what is the utility of the node, expected rewards after moving down the tree started from node $s$)\n",
    "\n",
    "Second term: exploration term (it ensures that nodes visited rarely, should be visited more in the future, not to miss good opportunities), $n(s)$ is number of times, the node was visited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9263f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Selection phase: the algorithm starts at the root node and moves down the tree by selecting the node with the best (highest) score.\n",
    "\n",
    "Expansion phase: if a node with an unexplored child is encountered, the algorithm creates a new child.\n",
    "\n",
    "Simulation: a random playout is performed to estimate the utility of the current node (from where the rollout starts).\n",
    "\n",
    "Backup: an averaging backup is used to update the value estimates of all nodes on the path from the root to the new node (from where the rollout started). The number of visits updated: $n(s) \\leftarrow n(s) + 1$. The utility is updated: $U(s) \\leftarrow U(s) + \\frac{R - (U(s))}{n(s)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa664a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notes on the last formula: \n",
    "\n",
    "$$\\overline{X} = \\frac{1}{n}\\sum_i{X_i}$$\n",
    "\n",
    "We can calculate the new average on-the-fly, when a new sample (R) comes:\n",
    "\n",
    "$$\\overline{X} \\leftarrow \\frac{n\\overline{X} + R}{n+1} = \\frac{(n+1)\\overline{X} + R - \\overline{X}}{n+1} = \\overline{X} + \\frac{R-\\overline{X}}{n+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6cb426",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bellman-equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b0d8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "we have learned how reinforcement learning is about leaning a good (or optimal) path of actions in situations of sequential decision making\n",
    "\n",
    "The \"simplest\" way of solving this problem is sampling all actions \n",
    "\n",
    "As there are more actions complexity explodes \n",
    "\n",
    "Remainder of the class is about how to deal with such complexity\n",
    "- Efficiently sampling (learning from previous data) (e.g. off-policy learning/ causality)\n",
    "- Efficiently finding \"optimal\" policies (e.g. Bellman Equations) \n",
    "- Efficiently summarizing what we have \"learned\" (e.g. deep reinforcement learning) and generalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc64fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Efficiency of the Bellman Equation - dynamic programming \n",
    "\n",
    "Lets denote the number of states with $n_s$ and the number of actions with $n_a$.\n",
    "The dynamic programming method is guaranteed to find the optimal policy in **polynomial time** (of $n_s$ and $n_a$).\n",
    "The total number of possible policies are $n_a^{n_s}$. Therefore the dynamic programming approach is **exponentially faster than any direct search e.g. Monte Carlo Tree search** in policy space.\n",
    "\n",
    "\n",
    "In practice, the dynamic programming approach can solve MDP problems (to be defined in what follows) with **millions of states**. This is important because if you manage to formalize  a problem in a tabular setting than it can be solved by dynamic programming and you can take advantage of its good convergence properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02fa54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## State-value, action-value\n",
    "\n",
    "**V - state-value function**\n",
    "\n",
    "**Q - action-value function**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ee34e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### State-value function (shortly: value-function)\n",
    "\n",
    "The $\\pi$ policy generates trajectories ($\\tau$) until the end of the episode, starting from $s_0$.\n",
    "$\\tau = [s_0, a_0, r_0, s_1, a_1, r_1, ..., s_i, a_i, r_i, ..., s_T, a_T, r_T]$ \n",
    "\n",
    "$$V^\\pi(s) = E_\\tau \\left[ G(\\tau) | s_0 = s, \\pi \\right]$$\n",
    "\n",
    "Where $G$ is the return. If the discounted return is used:\n",
    "\n",
    "$$V^\\pi(s) = E_\\tau \\left[ \\sum_i{\\gamma^i r_i} | s_0 = s, \\pi, r_i \\in \\tau \\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13ba51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Action-value function\n",
    "\n",
    "The $\\pi$ policy generates trajectories ($\\tau$) until the end of the episode, starting from $s_0$.\n",
    "$\\tau = [s_0, a_0, r_0, s_1, a_1, r_1, ..., s_i, a_i, r_i, ..., s_T, a_T, r_T]$ \n",
    "\n",
    "$$Q^\\pi(s, a) = E_\\tau \\left[ G(\\tau) | s_0 = s, a_0 = a, \\pi \\right]$$\n",
    "\n",
    "Where $G$ is the return. If the discounted return is used ($\\gamma < 1$):\n",
    "\n",
    "$$Q^\\pi(s, a) = E_\\tau \\left[ \\sum_i{\\gamma^i r_i} | s_0 = s, a_0 = a, \\pi, r_i \\in \\tau \\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b481bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a7a8d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most naiv approach would be sampling trajectories and calculate the returns for each of them. Then it is possible to average them out and find the value function for each state:\n",
    "\n",
    "$$V(s) = \\frac{\\sum_i^n{G(\\tau_i)}}{n}$$\n",
    "\n",
    "However, we can use a trajectory for sampling return for all the states encountered during the trajectory.\n",
    "There are two strategies how to do that: first visit and every visit.\n",
    "\n",
    "First visit means, if a state encountered several times during a trajectory, the return is calculated for only the first visit.\n",
    "\n",
    "Every visit means, if a state encountered several times, the returns are calculated for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fe5ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1uboWLi-NoQ1GMUZtrF1DBsc4Rxqx1BTU\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4de46d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The $s_i$ and $s_j$ is the same state but encountered after $i$ and $j$ steps.\n",
    "The return gathered after state $s_i$:\n",
    "\n",
    "$$G_i = \\sum_{k=i}^T{ r(s_k) }$$\n",
    "\n",
    "$$G_j = \\sum_{k=j}^T{ r(s_k) }$$\n",
    "\n",
    "For the first-visit MC, only $G_i$ is considered while for every-visit MC both $G_i$ and $G_j$ is considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861cc11a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can dynamic programming really help?\n",
    "\n",
    "By \"memorizing states\" one can try to solve RL problems by dynamic programming.\n",
    "\n",
    "But is this really feasible?\n",
    "\n",
    "**Time Complexity**\n",
    "In Dynamic programming problems, Time Complexity is the number of unique states/subproblems * time taken per state.\n",
    "\n",
    "In this problem, for a given n, there are n unique states/subproblems. For convenience, each state is said to be solved in a constant time. Hence the time complexity is O(n * 1).\n",
    "\n",
    "**Space Complexity**\n",
    "We use one array called cache to store the results of n states. Hence the size of the array is n. Therefore the space complexity is O(n).\n",
    "\n",
    "\n",
    "Note that there are trade-offs between complexity in space and time\n",
    "\n",
    "\n",
    "For large problems (many states and many actions) the Bellman formulation is still not efficient.\n",
    "\n",
    "[source](https://www.freecodecamp.org/news/demystifying-dynamic-programming-24fbdb831d3a/#:~:text=In%20Dynamic%20programming%20problems%2C%20Time,O(n%20*%201))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b172a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
