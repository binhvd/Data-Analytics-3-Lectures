{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9424bfb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Value-based RL DNN approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6a61e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DQN - Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11103a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The DQN algorithm was one of the first RL algorithms where the RL framework was combined with DNNs and the result was satisfying. The properties of the algorithm:\n",
    "\n",
    "* relatively easy to implement\n",
    "* simple but powerful\n",
    "* difficult convergence\n",
    "* sensitive to the hyper-parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a104260",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DQN is based on Q-learning but the $Q$-function is approximated by a neural network: $Q_\\theta(s, a)$. Then the update rule for the $\\theta$ parameters is given as:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\left( r_t + \\max_{a'}Q_\\theta(s', a') - Q_\\theta(s, a) \\right)\\cdot \\nabla_\\theta Q_\\theta(s, a)|_{\\theta = \\theta_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad7164",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How can we represent the state?** The problem is when something is moving on the image (e.g.: a ball) then a static frame is not able to represent it at a time point. The convolutional networks are memory less therefore they cannot store information across the consecutive frames. Remember, RL assumes an MDP (Markov decision process), which requires that the state contains all the information about the environment. \n",
    "\n",
    "A good approximation of this is to use a bunch of consecutive frames. Therefore a moving object appears in different places on the consecutive frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606c2a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Preprocessing steps:**\n",
    "\n",
    "The frames arriving from the simulator need to be preprocessed before feeding  them into the network.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1v6xXmKxSbElHF8RDgwmjP4eou2DxMHDj\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa4ba4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The preprocessing of the raw input frames consists of the following steps, as the above image illustrates:\n",
    "\n",
    "* grayscale the image\n",
    "* cropping (only the interesting part of the image will remain)\n",
    "* downsampling (or resizing) the image for $84\\times 84$\n",
    "* stacking four frames together to form the state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9a4ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Network architecture:**\n",
    "\n",
    "* Conv2D(kernel\\_num=32, kernel\\_size=(8, 8), padding='valid', input_shape=(84, 84, 4), strides=(4, 4))\n",
    "* Activation('relu')\n",
    "* Conv2D(kernel\\_num=64, kernel\\_size=(4, 4), padding='valid', strides=(2, 2))\n",
    "* Activation('relu')\n",
    "* Conv2D(kernel\\_num=64, kernel\\_size=(3, 3), padding='valid', strides=(1, 1))\n",
    "* Activation('relu')\n",
    "* Flatten()\n",
    "* Dense(units=512, activation='relu')\n",
    "* Dense(num\\_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead7d52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "DQN has two major tricks to avoid the instability caused by the neural network approximator:\n",
    "\n",
    "1. experience replay\n",
    "2. iterative update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2058fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Experience replay:** Use past experience for the same state/ action (not just the current one) - may have different rewards and different new states they result in\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1rqQQyPxhDTSFScMwmXDLGAE6eUpAWoSu\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ed75c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are two main reasons why experience replay can help to converge faster:\n",
    "\n",
    "1. If all of the samples are taken consecutively before feeding it into the network then the data samples will be correlated. This correlation makes the learning slower and harms the generalization. The replay buffer gathers the experiences in a buffer and the training batches are sampled according to a uniform distribution.\n",
    "2. There are valuable states (experiences) which should be used more times because it affects the policy strongly. However, may be the state is visited rarely because it is hard to reach it. Because the replay buffer stores a long history of experiences, the rare experiences can be reused several times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2063a1",
   "metadata": {},
   "source": [
    "In form of the memory buffer, we have direct control over the memory requirements of the algorithm, thus, we can __trade off stability for memory consumption__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb91638",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Iterative update:**\n",
    "\n",
    "One of the reasons behind the instability of Q-learning combined with a deep neural network is the fast change (high variance) of the one-step return. The one-step return depends on the network itself and the network weight is updated frequently. The network has no time to adapt and follow up changes.\n",
    "\n",
    "Iterative update or (delayed update) uses two networks for representing the $Q$-function. The architecture is the same but the weights are different. The weights are synchronized after a given number of steps.\n",
    "\n",
    "The goal of the first network is to calculate the return and it is not updated until synchronization. The second network is responsible for selecting the next step and it is always updated according to the update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154eb8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The update rule changes to the following one:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\cdot \\left( r_t + \\max_{a'}\\hat{Q}_{\\theta^-}(s', a') - Q_\\theta(s, a) \\right)\\cdot \\nabla_\\theta Q_\\theta(s, a)|_{\\theta = \\theta_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b56add",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The next slide shows the pseudo code of the DQN algorithm. The experience replay is the $D$ buffer in the code. The algorithm stores and samples experiences from the buffer. The iterative update is implemented with $\\hat{Q}$ and $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebc110",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1EaDj9o9-ACuMsf9PtmMw4p3CMtknTVAg\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15491a7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Video playing Atari](https://www.youtube.com/watch?v=V1eYniJ0Rnk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9108d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Disadvantages of value-based RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f5a139",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we have already seen, value-based RL derives the policy from a value-function:\n",
    "\n",
    "With Q-function:\n",
    "\n",
    "$$\\pi(s) = \\arg \\max_a{ Q(s, a) }$$\n",
    "\n",
    "When V-function is given:\n",
    "\n",
    "$$\\pi(s) = \\arg\\max_a \\left( T(s, a, s') \\cdot \\left[ r(s, a) + \\gamma V(s') \\right]\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06545023",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Due to the $\\arg \\max$ function the resulting policy is deterministic. However, in the first session, we have seen an example that stochastic policies have an advantage over deterministic ones. Let's recall the example:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1b-EDUk5cFVpqtOvZ0o4begzKCYwO0dMS\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4215e27f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here the example shows that cells with the question marks are the same for the agent. This is because here the state is defined by walls the agent can see around itself. Therefore the problem cannot be solved with a deterministic policy as efficiently as a stochastic one can. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16ee93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One main disadvantage of value-based RL is that it cannot calculate a stochastic policy directly. Furthermore changing the value-function causes unpredictable changes in the derived policy.\n",
    "\n",
    "Summarized - value based RL:\n",
    "* naturally gives a **deterministic policy**\n",
    "* **small changes** in the value function can cause severe changes in the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780005e",
   "metadata": {},
   "source": [
    "# Policy gradients\n",
    "\n",
    "- Basic idea: adjust the policy in the direction of the gradient (that makes it better)\n",
    "\n",
    "To overcome the limitations of value-based RL, the policy-based methods parametrize directly the policy $\\pi(s, a, \\theta)$. \n",
    "The question is how to optimize the parameters of the policy.\n",
    "As throughout in RL and machine learning, the optimization’s steps are done in the direction of the gradient of a loss function (or objective function). The question is how we can define a good objective function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437d475",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One example of Policy Gradient methods: REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88282e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1Il8lf_096OKqwVpZ6ohUIdYsjr-dtGHk\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cfd35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here the return is calculated with Monte Carlo, what we seen in the first section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e432df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Taxonomy of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2964c38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1Gz0WBOtTxYrZ91uAidFE_Tuw0RBSfl9O\" width=55%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c2a9d",
   "metadata": {},
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=11JF9TQok7WgDkqtOklY63RGJ6DhjvY_p\" width=55%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e33e1",
   "metadata": {},
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1OQXkGuNHx5dCThkWRhyVsiP_SjLhEN8v\" width=55%>\n",
    "\n",
    "\n",
    "\"Figure 1: Growth of published reinforcement learning papers. Shown are the number of RL-related publications (y-axis) per year (x-axis) scraped from Google Scholar searches.\" [source](https://www.researchgate.net/publication/319953023_Deep_Reinforcement_Learning_That_Matters/figures?lo=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
