{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation:\n",
    "\n",
    "- GAN training (as we saw) is __really hard__\n",
    "- Mode collapse, instability,...\n",
    "\n",
    "Let's recall the GAN setting:\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1TbebAOoZPw2TEhs2cFEntHI4f_r4LUxY\" width=45%>\n",
    "\n",
    "The observation behind this is, that the generator has to do a lot in one step, since it starts from __random noise__, and in a __single forward pass__, it has to \"reparametrize\" that noise __into a full image__.\n",
    "\n",
    "What if we would try to decompose this problem into __something more iterative__, that is, what if a model would have to learn to (un)do a noising process incrementally, a little bit of a time?\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1cN6vbHV3A6PJ8dxjlWvNEYxuaF3XPXMI\"  width=45%>\n",
    "\n",
    "__The main intuition here is:__\n",
    "- removing a bit of noise is a relatively simple task (classical models could do it, denoising autoencoders, rings a bell?)\n",
    "- removing noise on a stronger level requires knowledge about the relationships on the image (to \"in-paint\", you need to understand, what is on the image)  \n",
    "- you can repeat this exercise many times with more and more noise\n",
    "\n",
    "[nice intro](https://www.youtube.com/watch?v=1CIpzeNxIhU)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1LlKa1W9Kjckqz5rL2qmU_8PhGvtWjayp\"  width=45%>\n",
    "\n",
    "## \"Denoising\" done wrong and right\n",
    "\n",
    "To elaborate on the previous points, denoising can be done with extremely simple methods, like a median filter.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=12br8RVbOAHxeVV9EFRI4-8955DpJQ8C8\"  width=45%>\n",
    "\n",
    "Observe, that in case of this trivial approach, we:\n",
    "- take into account the local neighbourhood information\n",
    "- we do it with a __really dumb__ \"filter\", that assumes that a local pixel is similar to the average of it's neighbors (so we do NOT understand the relationship, just guess the most trivial)\n",
    "- we incur significant loss in fidelity\n",
    "- If we would be to repeat this exercise, we would destroy the majority of information very quicky \n",
    "\n",
    "If we would want to do this right, we should treat \"denoising\" as a minimal sized \"inpainting\" task, that is to say: we should understand, what objects are on the image, and should put in pixel values consistent with that object to \"fill out\" the \"missing\" (in this case: overwritten by noise) parts.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=19tK7vCMrs6qYhtRtTVMnqRDQrH753ErB\">\n",
    "\n",
    "\n",
    "Takeaway: __A really good denoiser understands the (visual) world.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viSIe7x4O462"
   },
   "source": [
    "# 1 Basic idea \n",
    "\n",
    "\n",
    "Based on:[Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd5eB_wkfDSp"
   },
   "source": [
    "## 1.1 Repeatedly add Guassian noise (noise from a normal distribution)\n",
    "- Add Gaussian noise repeatedly to an image \n",
    " - Called a forward noising process\n",
    "- If you do it frequently enough will end up with purely Gaussian noise (can be mathematically proven)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=107XsECWx7oK4C6xJon9vUWsiGqGd-ezc\" width=600 heigth=600>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5VzmrNcQ2nZ"
   },
   "source": [
    "## 1.2 Model to denoise back to (original) image \n",
    "- Task: If you give an image with some noise can you (the model) tell me what image (in the previouls noise step) it came from\n",
    "- Key aspect 1: denoising done one step at a time\n",
    "- Key aspect 2: same model repeatedly used for every step (given time step index as input as well)\n",
    "- If you can do this for the full process you are able to generate images from Gaussian noise\n",
    "- To do so model has to find out the noise in the image (added in the last step)\n",
    "- Makes large set of high quality training data out of every image\n",
    "- After training can sample noise from a random Gaussian distribution and get an image\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1UuCLTICGpIpqfuZzGAIRFr6ydtU_-Xji\" width=600 heigth=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWuR_NosdIt3"
   },
   "source": [
    "## 1.3 Relationship to previouls models\n",
    "- Like a GAN -> sampling from noise to get an image\n",
    "- Like Denoising Autoencoder - > adds noise and noise has to be removed ( but not in a step by step manner) \n",
    "- Like VAE - Each step in the difffusion model is variational inference (many small steps of variational inference)\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1ucQKiZWMkXeGDjgJtPjBG-fh4gMXDE0B\" width=600 heigth=600>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R0CNdLYe2sc"
   },
   "source": [
    "#2 The details "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j64ltZdfe5ts"
   },
   "source": [
    "## 2.1 Forward noising process: Sampling from noise\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1UuCLTICGpIpqfuZzGAIRFr6ydtU_-Xji\" width=600 heigth=600>\n",
    "\n",
    "\n",
    "Given a data distribution $x_0 \\sim q\\left(x_0\\right)$, we define a forward noising process $q$ which produces latents $x_1$ through $x_T$ by adding Gaussian noise at time $t$ with variance $\\beta_t \\in(0,1)$ as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q\\left(x_1, \\ldots, x_T \\mid x_0\\right) & :=\\prod_{t=1}^T q\\left(x_t \\mid x_{t-1}\\right) \\\\\n",
    "q\\left(x_t \\mid x_{t-1}\\right) & :=\\mathcal{N}\\left(x_t ; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t \\mathbf{I}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Given sufficiently large $T$ and a well behaved schedule of $\\beta_t$, the latent $x_T$ is nearly an [isotropic Gaussian distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic).\n",
    "\n",
    "Note that the above process is a **Markov chain**!\n",
    "\n",
    "Thus we are talking about an autogregressive model, as one step follows from the next step in a recursive manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LltYnXigqtO"
   },
   "source": [
    "**Explanation 1..**\n",
    "$$\n",
    "\\prod_{t=1}^T q\\left(x_t \\mid x_{t-1}\\right)\n",
    "$$\n",
    "Means that given the original image $x_0$ the noised images at point $x_1$... $x_t$ are obtaineed by repeatedly applying the noising process (this process is multiplicative), that is one can obtain the next step by multiplying the previouls step with the result of noise that is drawn and multiplying it with another draw of noise and so on..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqYNubWehgTg"
   },
   "source": [
    "**Explanation 2**\n",
    "$$\n",
    "q\\left(x_t \\mid x_{t-1}\\right):=\\mathcal{N}\\left(x_t ; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t \\mathbf{I}\\right)\n",
    "$$\n",
    "Distribution of next sample   $x_t$ from the previouls sample $x_{t-1}$ is a normal distribution   $\\mathcal{N}$  centered at (with mean) $\\sqrt{1-\\beta_t} x_{t-1}$ and variance $\\beta_t \\mathbf{I}$, which means that it is noise that has a diagonal covariance matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cy1MO9xZkswg"
   },
   "source": [
    "**Noise Schedule**\n",
    "\n",
    "$$\n",
    "\\beta_t\n",
    "$$\n",
    "is chosen through a noise schedule that we will come back to later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_B4Ru2olWIZ"
   },
   "source": [
    "**Scaling factor**\n",
    "$$\n",
    "\\alpha_t =1-\\beta_t\n",
    "$$\n",
    "\n",
    "is a scaling factor which will gradually shrinks the mean towards zero\n",
    "\n",
    "often rewritten as $$\\alpha_t$$ for simplification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ouEXvEFHRLL"
   },
   "source": [
    "**Closed form solution**\n",
    "For step ahead multiple time steps ahead in the noising process $\\bar{\\alpha}_t$  can be written as the cumulative product of the individual $\\alpha$ 's\n",
    "\n",
    "$$\n",
    "\\bar{\\alpha}_t=\\prod_{s=1}^t a_s\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-UpczkwK9d7"
   },
   "source": [
    "**Closed form solution**\n",
    "Instead of iteratively applying the noise image at a certain time step can also be derived in a closed form solution\n",
    "\n",
    "$$\n",
    "x_t=\\sqrt{\\bar{\\alpha}_t} x_0+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon\n",
    "$$\n",
    "\n",
    "This is important for coding efficiency, since it saves us from say, applying the same noising step 1000 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JLrwY9TZvmT"
   },
   "source": [
    "**Sampling**\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1QsDjyO9_bq97v4ViOFqqTeV4WsGVcIWB\" width=45%>\n",
    "\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/2006.11239.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_g1gItegqqP"
   },
   "source": [
    "## 2.2 Backward process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGaeEQjHwCcG"
   },
   "source": [
    "Thus, if we know the exact reverse distribution $q\\left(x_{t-1} \\mid x_t\\right)$, we can sample $x_T \\sim \\mathcal{N}(0, \\mathrm{I})$ and run the process in reverse to get a sample from $q\\left(x_0\\right)$. However, since $q\\left(x_{t-1} \\mid x_t\\right)$ depends on the entire data distribution, we approximate it using a neural network as follows:\n",
    "$$\n",
    "p_\\theta\\left(x_{t-1} \\mid x_t\\right):=\\mathcal{N}\\left(x_{t-1} ; \\mu_\\theta\\left(x_t, t\\right), \\Sigma_\\theta\\left(x_t, t\\right)\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGwvEUtdwzBJ"
   },
   "source": [
    "**Explanation 1: Why does the revers distribution depend on the entire dataset**\n",
    "Answer- Because to figure out what the noise part is you need to figure out the original underlying distribution of real images is\n",
    "\n",
    "Give me a distribution over images where this could have come from\n",
    "\n",
    "NN will produce mean and covariance matrix of the noise given the image\n",
    "\n",
    "What is the gaussian distribution of images where this came from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmKG7q8C1Zv_"
   },
   "source": [
    "## 2.2 Loss function and relationship to variantional auto encoders\n",
    "(no need to understand details of math here)\n",
    "The combination of $q$ and $p$ is a variational auto-encoder (Kingma \\& Welling, 2013), and we can write the variational lower bound (VLB) as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_{\\mathrm{vlb}} & :=L_0+L_1+\\ldots+L_{T-1}+L_T \\\\\n",
    "L_0 & :=-\\log p_\\theta\\left(x_0 \\mid x_1\\right) \\\\\n",
    "L_{t-1} & :=D_{K L}\\left(q\\left(x_{t-1} \\mid x_t, x_0\\right) \\| p_\\theta\\left(x_{t-1} \\mid x_t\\right)\\right) \\\\\n",
    "L_T & :=D_{K L}\\left(q\\left(x_T \\mid x_0\\right) \\| p\\left(x_T\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "...Like many mini step variational auto-encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wD2d6bz_17I7"
   },
   "source": [
    "**Explanation**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L_{t-1} & :=D_{K L}\\left(q\\left(x_{t-1} \\mid x_t, x_0\\right) \\| p_\\theta\\left(x_{t-1} \\mid x_t\\right)\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "distribution that the neural network outputs: \n",
    "$$p_\\theta\\left(x_{t-1} \\mid x_t\\right)$$\n",
    "\n",
    "match with what it actually is:\n",
    "$$q\\left(x_{t-1} \\mid x_t, x_0\\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHYQ9nZR3n8f"
   },
   "source": [
    "### 2.2.1 The Loss\n",
    "\n",
    "There are different objectives one could apply (see paper), but the objective of predicting where the noise is seems to be most effective.\n",
    "\n",
    "So all of the above tanslates in the simple loss function:\n",
    "$$\n",
    "L_{\\text {simple }}=E_{t, x_0, \\epsilon}\\left[\\left\\|\\epsilon-\\epsilon_\\theta\\left(x_t, t\\right)\\right\\|^2\\right]\n",
    "$$\n",
    "\n",
    "In the original paper the loss is also combined with an importance sampling scheme, as the earlier steps of the noise diffusion add much more loss than the previous steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7Jwe1CmIAKe"
   },
   "source": [
    "**Explanation**\n",
    "\n",
    "All this requires to predict he noise for every pixel and the be take the L2 norm between the predicted noise and the actual noise and average over it\n",
    "\n",
    "note that I am leaving out some important details about the loss here for simplification. To see these details, havea look at the original paper referenced at the beginning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrxCnQe-wXzH"
   },
   "source": [
    "# 2.3 The model\n",
    "\n",
    "\n",
    "The original model has a UNet achrchitecture \n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1Jm18gyEHXxr0iiF0DOGGYKrm3szA9lYP\" width=600 heigth=600>\n",
    "\n",
    "\n",
    "- see [here:](https://paperswithcode.com/method/u-net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8R9as7OVArC"
   },
   "source": [
    "## 2.4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6hkfi2PVD4A"
   },
   "source": [
    "<img src=\"http://drive.google.com/uc?export=view&id=1EbnLt-swQV7wlNZGRzL_17kRFh4Rz4hO\" width=45%>\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/2006.11239.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWjtUnsIP9MZ"
   },
   "source": [
    "# 3 Noise Schedule \n",
    "Set a cosine schedule so that noise is added faster at the beginning and more slowly at the end\n",
    "\n",
    "On the following picture the top is the linear noise schedule and the bottom is the cosine noise schedule\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=19vr1NTVF9LR33l5MgSgLCID93RkjBMUn\" width=45%>\n",
    "\n",
    "\n",
    "\n",
    "And the Fr√©chet inception score and fraction of diffusion process skipped\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=11a7PPp3JoGJFsIhYfma-AvlHsoWL1-Sk\" width=45%>\n",
    "\n",
    "Note that this analysis does not constitute super strong evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDz6MQd8wA2p"
   },
   "source": [
    "# 4 Classifier Guidance / conditioning\n",
    "\n",
    "Based on: [Diffusion Models beat gans on image Synthesis](https://arxiv.org/abs/2105.05233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzJJo1JEwJ90"
   },
   "source": [
    "- Use model for conditional image generation\n",
    "- Use Classifier to improve diffusion generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A7tLg6wwLkQ"
   },
   "source": [
    " - Pre-trained diffusion model can be conditioned using the gradients of a classifier\n",
    " - In particular, we can train a classifier $p_\\phi\\left(y \\mid x_t, t\\right)$ on noisy images $x_t$, and then use gradients $\\nabla_{x_t} \\log p_\\phi\\left(y \\mid x_t, t\\right)$ to guide the diffusion sampling process towards an arbitrary class label $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H09aX7aywLU0"
   },
   "source": [
    "Instead of simply reverting the noise process if the model is told what label that image is from/ what class the image is from, can it do a better job\n",
    "\n",
    "To condition on y\n",
    "\n",
    "$$\n",
    "p_{\\theta, \\phi}\\left(x_t \\mid x_{t+1}, y\\right)=Z p_\\theta\\left(x_t \\mid x_{t+1}\\right) p_\\phi\\left(y \\mid x_t\\right)\n",
    "$$\n",
    "\n",
    "where Z is a normalizing constant\n",
    "\n",
    "and the right term\n",
    "$$p_\\phi\\left(y \\mid x_t\\right)$$ is the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ePP32IX4RJz"
   },
   "source": [
    "Not that the the \"descriminator\" is activ at every step here.\n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1jIN-Gi5BDPXcoO2kDGDHQDI859x4zaD_\" width=600 heigth=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \"Detour\": generalizing discrimination\n",
    "\n",
    "Our aim is to condition the generation process on some kind of human given constraints. For that, a discriminative (or as we will see contrastive) model can come in handy.\n",
    "\n",
    "The recent advancements in image \"discrimination\" are directly useful in this case.\n",
    "\n",
    "## The CLIP model \n",
    "\n",
    "\n",
    "[Nice intro](https://www.youtube.com/watch?v=u0HG77RNhPE)\n",
    "\n",
    "\n",
    "- contrastive pretraining (a more mature form of contrastive learning on __large batches__)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1OryXOYhwrwKCFAFQEhl3mBvGE_YB8eit\" width=45%>\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1lUSK8wiMMVW9ozDQrMeFPsIr4dlWZOWD\" width=45%>\n",
    "\n",
    "source: [OpenAI's original post](https://openai.com/research/clip)\n",
    "\n",
    "\n",
    "- transformer (utilizing state of the art architectures)\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1xD5GIh19BkiuqBkb1hbbzEoCIlphGaYS\" width=45%>\n",
    "\n",
    "- Everything becomes a text task (points recent advancements in NLP)\n",
    "\n",
    "## Combination with generative models\n",
    "\n",
    "\"In the generative AI models for images created after DALL-E 1, CLIP often takes a central role, for example in CLIP+VQGAN, CLIP-guided diffusion, or StyleGAN-NADA. In these examples, CLIP computes the difference between an input text and an image generated by, say, a GAN. The difference is minimized by the model to produce a better image.\"\n",
    "\n",
    "[source](https://the-decoder.com/new-clip-model-aims-to-make-stable-diffusion-even-better/)\n",
    "\n",
    "The main advantage here is, that with this \"everything is a text task\" approach, we __open up the conditioning possibilities to general text__ beyond the pure categorical choices.  \n",
    "\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1MZ5OVIqY1YnwT2KPQLBU1Bi4N-1WTwPt\" width=45%>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJFu6QOElZyQ"
   },
   "source": [
    "# 5 Why diffusion models (may be) more effective than GANS\n",
    "\n",
    "**Advantages of diffusion models**\n",
    "- Smaller steps more iterative guided process from noise to Image\n",
    "- Does not suffer from the usual GAN convergence difficulties: Minimizing the loss will generally help, no competitive game theory settings\n",
    "- As a result of more detailed guidance no problems with mode collaps in the sense of outputting a bogus image that convinces the discriminator\n",
    "- But still have to make sure that the whole space of sampled e.g. through conditioning\n",
    "\n",
    "**Disadvantages of diffusion models**\n",
    "- Slow generation process due to the high number of iterative steps\n",
    "- No Dimension re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjGNk7BeG4vV"
   },
   "source": [
    "Some results for diffusion models, in this case from Midjourney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDQMCvaVHA1G"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "# Create a YouTubeVideo instance with the video ID.\n",
    "video = YouTubeVideo(\"twKgWGmsBLY\")\n",
    "\n",
    "# Display the video.\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern diffusion models\n",
    "\n",
    "After the initial success of diffucion models large scale work begun to reproduce and enhance their results. One of the most important models in this family is the OpenSource [Stable diffusion](https://stability.ai/blog/stable-diffusion-public-release).\n",
    "\n",
    "A very nice, detailed introduction can be found [here](https://jalammar.github.io/illustrated-stable-diffusion/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGbTdD5FUC0v"
   },
   "source": [
    "# Follow-up reads\n",
    "\n",
    "Note that the DDPM paper showed that diffusion models are a promising direction for (un)conditional image generation. This has since then (immensely) been improved, most notably for text-conditional image generation. Below, we list some important (but far from exhaustive) follow-up works:\n",
    "\n",
    "- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)): finds that learning the variance of the conditional distribution (besides the mean) helps in improving performance\n",
    "- Cascaded Diffusion Models for High Fidelity Image Generation ([Ho et al., 2021](https://arxiv.org/abs/2106.15282)): introduce cascaded diffusion, which comprises a pipeline of multiple diffusion models that generate images of increasing resolution for high-fidelity image synthesis\n",
    "- Diffusion Models Beat GANs on Image Synthesis ([Dhariwal et al., 2021](https://arxiv.org/abs/2105.05233)): show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models by improving the U-Net architecture, as well as introducing classifier guidance\n",
    "- Classifier-Free Diffusion Guidance ([Ho et al., 2021](https://openreview.net/pdf?id=qw8AKxfYbI)): shows that you don't need a classifier for guiding a diffusion model by jointly training a conditional and an unconditional diffusion model with a single neural network\n",
    "- Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) ([Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf)): use a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image\n",
    "- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) ([Saharia et al., 2022](https://arxiv.org/abs/2205.11487)): shows that combining a large pre-trained language model (e.g. T5) with cascaded diffusion works well for text-to-image synthesis\n",
    "\n",
    "Note that this list only includes important works until the time of writing, which is June 7th, 2022.\n",
    "\n",
    "For now, it seems that the main (perhaps only) disadvantage of diffusion models is that they require multiple forward passes to generate an image (which is not the case for generative models like GANs). However, there's [research going on](https://arxiv.org/abs/2204.13902) that enables high-fidelity generation in as few as 10 denoising steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-8PzefeQ4Fu"
   },
   "source": [
    "Further Reading:\n",
    "1. Paper: https://arxiv.org/pdf/1503.03585.pdf\n",
    "2. Paper: https://arxiv.org/pdf/2006.11239.pdf\n",
    "3. Paper: https://arxiv.org/pdf/2102.09672.pdf\n",
    "4. Paper: https://arxiv.org/pdf/2105.05233.pdf\n",
    "5. CFG: https://arxiv.org/pdf/2207.12598.pdf\n",
    "6. Timestep Embedding: https://machinelearningmastery.com/a-\n",
    "\n",
    "\n",
    "**Sources:**\n",
    "- Github implementation [Denoising Diffusion Pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch)\n",
    "- Niels Rogge, Kashif Rasul, [Huggingface notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=3a159023)\n",
    "- Papers on Diffusion models ([Dhariwal, Nichol, 2021], [Ho et al., 2020] ect.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfxfhV6yQ3qR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPIjUWCTWkqvDtfMaVTOl6I",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
